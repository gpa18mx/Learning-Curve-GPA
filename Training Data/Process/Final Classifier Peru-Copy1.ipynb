{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToolKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadexcel(x):#, #y = 'Sheet1'):\n",
    "    run = True\n",
    "    file= pd.ExcelFile(x)\n",
    "    names = file.sheet_names\n",
    "    names = re.sub(\"'\",'', str(names))\n",
    "    print (\"This file contains this tab names:\", names )\n",
    "    while run is True:\n",
    "        try:\n",
    "            decision = (input(\"Choose your tab name? \"))\n",
    "            print (\"Your choice is\", decision)\n",
    "            confirm = int(input(\"This is correct? Please write 1 for yes or 2 for no \"))\n",
    "            if confirm == 1:\n",
    "                print (\"Processing Tab, please wait\")\n",
    "                final_file = file.parse(decision)\n",
    "                return final_file\n",
    "                run = False\n",
    "            else:\n",
    "                print ('Choose the right tab!') \n",
    "        except Exception as e:\n",
    "            print (\"There is an error: \", e)\n",
    "            print (\"Please Try Again! :)\")\n",
    "            continue\n",
    "            \n",
    "def loading_excel():\n",
    "    x = input('\\033[94m'+'Please write your file location: ' +'\\033[0m')\n",
    "    real_x = path.realpath(x)\n",
    "    doc = loadexcel(real_x) \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergefield(x, jo=\"\"):\n",
    "    bold = \"\\033[1m\"\n",
    "    end =  \"\\033[0m\"\n",
    "    GREEN = '\\033[92m'\n",
    "    i=-1\n",
    "    names = list(x)\n",
    "    b = dict(enumerate(names))\n",
    "    print (\"This file contains this columns:\" )\n",
    "    for name in names:\n",
    "        i=i+1\n",
    "        print(str(i)+\".\"+\" \"+ name)\n",
    "    print (bold+GREEN+\"Choose the columns you want to merge.\"+end)\n",
    "    election = input().split(\",\")\n",
    "    item = list(map(int, election))\n",
    "    convert = [b[y] for y in item]\n",
    "    print(\"Your Choice is\", convert)\n",
    "    stripped = x[convert].fillna('').apply(lambda x: x.str.strip())\n",
    "    join_symb = jo\n",
    "    stripped = stripped.astype(str).apply(join_symb.join, axis=1)\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filtersame(x,y):  \n",
    "    words_equal = []\n",
    "    for word in x:\n",
    "        if word in y:\n",
    "            words_equal += [word]\n",
    "    return words_equal\n",
    "\n",
    "def removequal(x,y):  \n",
    "    words_dif = []\n",
    "    for word in y:\n",
    "        if word not in x:\n",
    "            words_dif += [word]\n",
    "    return words_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #create a variable to delete common words\n",
    "\n",
    "def stopwords_list( lan ='spanish'):\n",
    "    lan = str(lan)\n",
    "    stopwords_es = stopwords.words(lan)\n",
    "    stop_es = pd.DataFrame(stopwords_es)\n",
    "    stop_es.columns =['StopWords']\n",
    "    es_pal = '\\\\b'+stop_es['StopWords'].str.lower().astype(str)+'\\\\b'\n",
    "    es_pal = es_pal.tolist()\n",
    "    return es_pal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Clean Historical DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import re #regex\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write your file location: C:\\Users\\Gpardo\\Desktop\\Clasification\\DB\\Peru\\DB Historical Peru.xlsx\n",
      "This file contains this tab names: [Sheet1]\n",
      "Choose your tab name? sheet1\n",
      "Your choice is sheet1\n",
      "This is correct? Please write 1 for yes or 2 for no 1\n",
      "Processing Tab, please wait\n",
      "There is an error:  No sheet named <'sheet1'>\n",
      "Please Try Again! :)\n",
      "Choose your tab name? Sheet1\n",
      "Your choice is Sheet1\n",
      "This is correct? Please write 1 for yes or 2 for no 1\n",
      "Processing Tab, please wait\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timescale</th>\n",
       "      <th>DESCRIPCION COMERCIAL 1</th>\n",
       "      <th>NOMBRE COMERCIAL</th>\n",
       "      <th>MARCA</th>\n",
       "      <th>MODELO</th>\n",
       "      <th>CARACTERISTICAS</th>\n",
       "      <th>Processed</th>\n",
       "      <th>Vendor</th>\n",
       "      <th>Product</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>CPU Type</th>\n",
       "      <th>New Vendor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>RAID SHIELD RAID SHIELD S/M</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>AGUA DE COCO, SIGMA ALDRICH, S/M</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>SERVIDOR DE COMPUTO DELL R530</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>REFRIGERADOR, ARCTIKO, PR1400</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>ENZIMAS DE LABORATORIO, NEW ENGLAND BIOLABS, S/M</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Timescale                            DESCRIPCION COMERCIAL 1  \\\n",
       "0    2018Q1                       RAID SHIELD RAID SHIELD S/M    \n",
       "1    2018Q1                  AGUA DE COCO, SIGMA ALDRICH, S/M    \n",
       "2    2018Q1                     SERVIDOR DE COMPUTO DELL R530    \n",
       "3    2018Q1                     REFRIGERADOR, ARCTIKO, PR1400    \n",
       "4    2018Q1  ENZIMAS DE LABORATORIO, NEW ENGLAND BIOLABS, S/M    \n",
       "\n",
       "  NOMBRE COMERCIAL MARCA MODELO CARACTERISTICAS Processed Vendor Product  \\\n",
       "0                                                   Trash    NaN     NaN   \n",
       "1                                                   Trash    NaN     NaN   \n",
       "2                                                   Trash    NaN     NaN   \n",
       "3                                                   Trash    NaN     NaN   \n",
       "4                                                   Trash    NaN     NaN   \n",
       "\n",
       "  Model Name CPU Type New Vendor  \n",
       "0        NaN      NaN        NaN  \n",
       "1        NaN      NaN        NaN  \n",
       "2        NaN      NaN        NaN  \n",
       "3        NaN      NaN        NaN  \n",
       "4        NaN      NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#C:\\Users\\Gpardo\\Desktop\\Clasification\\DB\\Peru\\DB Historical Peru.xlsx\n",
    "db_hist = loading_excel() #Modelo\n",
    "db_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timescale                                                      2017Q4\n",
       "DESCRIPCION COMERCIAL 1    UNIDAD DE PROCESO DIGITAL, LENOVO, S28600 \n",
       "NOMBRE COMERCIAL                         ,,UNIDAD DE PROCESO DIGITAL \n",
       "MARCA                                                        ,LENOVO \n",
       "MODELO                                                        S28600 \n",
       "CARACTERISTICAS                Desktop TC M710q I56400T 8G 1TB W10DG \n",
       "Processed                                                       Trash\n",
       "Vendor                                                            NaN\n",
       "Product                                                           NaN\n",
       "Model Name                                                        NaN\n",
       "CPU Type                                                          NaN\n",
       "New Vendor                                                        NaN\n",
       "Name: 30690, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_db = db_hist.copy()\n",
    "working_db['DESCRIPCION COMERCIAL 1'] = working_db['DESCRIPCION COMERCIAL 1'].str.replace(r\"^(\\W+)\",\"\",case=False)\n",
    "working_db['DESCRIPCION COMERCIAL 1'] = working_db['DESCRIPCION COMERCIAL 1'].str.replace(r\"\\, \\,\",\", \",case=False)\n",
    "//\n",
    "#working_db[['Descripcion', 'Marca','Modelo']] = working_db['DESCRIPCION COMERCIAL 1'].str.split(',', 2, expand=True)\n",
    "working_db.iloc[30690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sans_comma = working_db[~working_db['DESCRIPCION COMERCIAL 1'].str.contains('(\\w)+(,)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gpardo\\Desktop\\Clasification\\DB\\Peru\\DB_Revision_18_07_16.xlsx\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "trash_out = r'C:\\Users\\Gpardo\\Desktop\\Clasification\\DB\\Peru\\DB_Revision' \n",
    "save = datetime.datetime.now().strftime(\"%y_%m_%d\")\n",
    "dir = trash_out +\"_\"+str(save)+\".xlsx\"\n",
    "print(dir)\n",
    "\n",
    "writer = pd.ExcelWriter(dir, engine='xlsxwriter')\n",
    "\n",
    "#Convert the dataframe to an XlsxWriter Excel object.\n",
    "working_db.to_excel(writer, sheet_name='Final Database')\n",
    "sans_comma.to_excel(writer, sheet_name='Sin Coma')\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file contains this columns:\n",
      "0. Timescale\n",
      "1. DESCRIPCION COMERCIAL 1\n",
      "2. NOMBRE COMERCIAL\n",
      "3. MARCA\n",
      "4. MODELO\n",
      "5. CARACTERISTICAS\n",
      "6. Processed\n",
      "7. Vendor\n",
      "8. Product\n",
      "9. Model Name\n",
      "10. CPU Type\n",
      "11. New Vendor\n",
      "12. Working\n",
      "\u001b[1m\u001b[92mChoose the columns you want to merge.\u001b[0m\n",
      "1,5\n",
      "Your Choice is ['DESCRIPCION COMERCIAL 1', 'CARACTERISTICAS']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timescale</th>\n",
       "      <th>DESCRIPCION COMERCIAL 1</th>\n",
       "      <th>NOMBRE COMERCIAL</th>\n",
       "      <th>MARCA</th>\n",
       "      <th>MODELO</th>\n",
       "      <th>CARACTERISTICAS</th>\n",
       "      <th>Processed</th>\n",
       "      <th>Vendor</th>\n",
       "      <th>Product</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>CPU Type</th>\n",
       "      <th>New Vendor</th>\n",
       "      <th>Working</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>RAID SHIELD RAID SHIELD S M</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAID SHIELD RAID SHIELD S M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>AGUA DE COCO SIGMA ALDRICH S M</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AGUA DE COCO SIGMA ALDRICH S M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>SERVIDOR DE COMPUTO DELL R530</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SERVIDOR DE COMPUTO DELL R530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>REFRIGERADOR ARCTIKO PR1400</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REFRIGERADOR ARCTIKO PR1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>ENZIMAS DE LABORATORIO NEW ENGLAND BIOLABS S M</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Trash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENZIMAS DE LABORATORIO NEW ENGLAND BIOLABS S M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Timescale                          DESCRIPCION COMERCIAL 1 NOMBRE COMERCIAL  \\\n",
       "0    2018Q1                     RAID SHIELD RAID SHIELD S M                     \n",
       "1    2018Q1                  AGUA DE COCO SIGMA ALDRICH S M                     \n",
       "2    2018Q1                   SERVIDOR DE COMPUTO DELL R530                     \n",
       "3    2018Q1                     REFRIGERADOR ARCTIKO PR1400                     \n",
       "4    2018Q1  ENZIMAS DE LABORATORIO NEW ENGLAND BIOLABS S M                     \n",
       "\n",
       "  MARCA MODELO CARACTERISTICAS Processed Vendor Product Model Name CPU Type  \\\n",
       "0                                  Trash    NaN     NaN        NaN      NaN   \n",
       "1                                  Trash    NaN     NaN        NaN      NaN   \n",
       "2                                  Trash    NaN     NaN        NaN      NaN   \n",
       "3                                  Trash    NaN     NaN        NaN      NaN   \n",
       "4                                  Trash    NaN     NaN        NaN      NaN   \n",
       "\n",
       "  New Vendor                                          Working  \n",
       "0        NaN                     RAID SHIELD RAID SHIELD S M   \n",
       "1        NaN                  AGUA DE COCO SIGMA ALDRICH S M   \n",
       "2        NaN                   SERVIDOR DE COMPUTO DELL R530   \n",
       "3        NaN                     REFRIGERADOR ARCTIKO PR1400   \n",
       "4        NaN  ENZIMAS DE LABORATORIO NEW ENGLAND BIOLABS S M   "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#working_db[working_db['DESCRIPCION COMERCIAL 1']=='TABLETS']\n",
    "working_db['Working'] = mergefield(working_db, jo=\" \") #1,5\n",
    "working_db['Working'] = working_db['Working'].str.replace(r\"\\W\",\" \",case=False)\n",
    "working_db['Working'] = working_db['Working'].str.replace(r\" +\",\" \",case=False)\n",
    "working_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Trash', 0: 'Model'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timescale</th>\n",
       "      <th>Working</th>\n",
       "      <th>Processed</th>\n",
       "      <th>Category Codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>RAID SHIELD RAID SHIELD S M</td>\n",
       "      <td>Trash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>AGUA DE COCO SIGMA ALDRICH S M</td>\n",
       "      <td>Trash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>SERVIDOR DE COMPUTO DELL R530</td>\n",
       "      <td>Trash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>REFRIGERADOR ARCTIKO PR1400</td>\n",
       "      <td>Trash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018Q1</td>\n",
       "      <td>ENZIMAS DE LABORATORIO NEW ENGLAND BIOLABS S M</td>\n",
       "      <td>Trash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Timescale                                          Working Processed  \\\n",
       "0    2018Q1                     RAID SHIELD RAID SHIELD S M      Trash   \n",
       "1    2018Q1                  AGUA DE COCO SIGMA ALDRICH S M      Trash   \n",
       "2    2018Q1                   SERVIDOR DE COMPUTO DELL R530      Trash   \n",
       "3    2018Q1                     REFRIGERADOR ARCTIKO PR1400      Trash   \n",
       "4    2018Q1  ENZIMAS DE LABORATORIO NEW ENGLAND BIOLABS S M      Trash   \n",
       "\n",
       "   Category Codes  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_db2 = working_db[['Timescale','Working','Processed']]\n",
    "working_db2['Category Codes'] = working_db2['Processed'].astype('category').cat.codes\n",
    "model_dict = working_db2[['Processed','Category Codes']].set_index('Category Codes')['Processed'].to_dict()\n",
    "print(model_dict)\n",
    "working_db2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esp_list= stopwords_list()\n",
    "models_word['Words'] = models_word['Words'].str.replace('|'.join(esp_list), \"\", case=False).str.replace('\\s\\s+', \" \", case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem Only Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "stemmer.stem('cuando')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#db_hist_trash['Descripcion_Clean'] = db_hist_trash['Descripcion_Clean'].apply(lambda x : filter(None,x.split(\" \"))).apply(lambda x : [stemmer.stem(y) for y in x]).apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating 1st type classifer with encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding information 0 is for trash & 1 is for possible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "Trash_codes_org = pd.DataFrame(db_hist_trash['Clean Line'].str.lower().unique(), columns=['Words'])\n",
    "Trash_codes_org['Encode'] = 0\n",
    "Trash_codes_org['Type'] = 'Trash'\n",
    "Trash_codes_org = Trash_codes_org[~Trash_codes_org['Words'].str.contains(\"tablet\")]\n",
    "Trash_codes_org['Words'] = Trash_codes_org['Words']#.apply(lambda x : filter(None,x.split(\" \"))).apply(lambda x : [stemmer.stem(y) for y in x]).apply(lambda x : \" \".join(x))\n",
    "Trash_codes_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_word = pd.DataFrame(db_hist_tablet['Clean Line'].str.lower().unique(),columns=['Words'])\n",
    "models_word['Encode'] = 1\n",
    "models_word['Type'] = 'Tablets'\n",
    "models_word['Words'] = models_word['Words'].str.replace(r\"\\s\\s\",\" \",case=False) #.apply(lambda x : filter(None,x.split(\" \"))).apply(lambda x : [stemmer.stem(y) for y in x]).apply(lambda x : \" \".join(x))\n",
    "models_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_word = pd.DataFrame(db_hist_pc['Clean Line'].str.lower().unique(),columns=['Words'])\n",
    "pc_word['Encode'] = 2\n",
    "pc_word['Type'] = 'PC'\n",
    "pc_word['Words'] = pc_word['Words'].str.replace(r\"\\s\\s\",\" \",case=False)#.apply(lambda x : filter(None,x.split(\" \"))).apply(lambda x : [stemmer.stem(y) for y in x]).apply(lambda x : \" \".join(x))\n",
    "pc_word.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Removing same words & StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_list= stopwords_list()\n",
    "models_word['Words'] = models_word['Words'].str.replace('|'.join(esp_list), \"\", case=False).str.replace('\\s\\s+', \" \", case=False)\n",
    "Trash_codes_org['Words'] = Trash_codes_org['Words'].str.replace('|'.join(esp_list), \"\", case=False).str.replace('\\s\\s+', \" \", case=False)\n",
    "pc_word['Words'] = pc_word['Words'].str.replace('|'.join(esp_list), \"\", case=False).str.replace('\\s\\s+', \" \", case=False)\n",
    "models_word.head()\n",
    "pc_word.head()\n",
    "Trash_codes_org.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_db = models_word.append(Trash_codes_org)\n",
    "unique_db['Words'] = unique_db['Words'].str.lower()\n",
    "print(len(unique_db))\n",
    "unique_db.head()\n",
    "#unique_db[unique_db['Words'].str.contains('tablets')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_db_org = models_word.append(Trash_codes_org)\n",
    "unique_db_org = unique_db_org.append(pc_word)\n",
    "unique_db_org['Words'] = unique_db_org['Words'].str.lower()\n",
    "print(len(unique_db_org))\n",
    "unique_db_org.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rearrange Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "unique_db_org = unique_db_org.reset_index(drop=True)\n",
    "df = unique_db_org.reindex(np.random.permutation(unique_db_org.index))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words contained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df[['Words','Type']].groupby('Type').count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model & Trash Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each line is a class\n",
    "#mod = unique_db[unique_db['Encode']==1]['Words'].tolist()\n",
    "#tr = unique_db[unique_db['Encode']==0]['Words'].tolist()\n",
    "tablets = unique_db_org[unique_db_org['Encode']==1]['Words'].tolist()\n",
    "#tablets = unique_db_org[unique_db_org['Encode']==1]['Words'].apply(lambda x : filter(None,x.split(\" \"))).apply(lambda x : [stemmer.stem(y) for y in x]).apply(lambda x : \" \".join(x)).tolist()\n",
    "pcs = unique_db_org[unique_db_org['Encode']==2]['Words'].tolist()\n",
    "#pcs = unique_db_org[unique_db_org['Encode']==2]['Words'].apply(lambda x : filter(None,x.split(\" \"))).apply(lambda x : [stemmer.stem(y) for y in x]).apply(lambda x : \" \".join(x)).tolist()\n",
    "tr = unique_db_org[unique_db_org['Encode']==0]['Words'].tolist()\n",
    "#tr = unique_db_org[unique_db_org['Encode']==0]['Words'].apply(lambda x : filter(None,x.split(\" \"))).apply(lambda x : [stemmer.stem(y) for y in x]).apply(lambda x : \" \".join(x)).tolist()\n",
    "tab_str = \" \".join(tablets)\n",
    "pcs_str = \" \".join(pcs)\n",
    "tr_str = \" \".join(tr)\n",
    "tr_str\n",
    "tab_str\n",
    "pcs_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [tab_str]+[pcs_str]+[tr_str]\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_test = loading_excel() #C:\\Users\\Gpardo\\Desktop\\Consumption\\Python Projects\\Imports\\Chile\\Final\\DB Cleaning_test_18_06_13.xlsx\n",
    "db_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_test['Index Field 1'] = mergefield(db_test,\" \") #11,12,13,14\n",
    "db_test['Index Field 1'] = db_test['Index Field 1'].str.replace('|'.join(esp_list), \"\", case=False).str.replace('\\s\\s+', \" \", case=False)\n",
    "db_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_db = db_test[['Index Field 1','Type']]\n",
    "test_db.loc[test_db['Type']==\"Trash\",'Encode']= \"0\"\n",
    "test_db.loc[test_db['Type']==\"Tablet\",'Encode']= \"1\"\n",
    "test_db.loc[test_db['Type']==\"PC\",'Encode']= \"2\"\n",
    "\n",
    "#sample_imp.loc[sample_imp['Process'].str.contains(\"Possible Models\"), 'Codes Partial Match1'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df['Words']\n",
    "y_train = df['Encode']\n",
    "X_test = test_db['Index Field 1']\n",
    "y_test = test_db['Encode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Porcentaje de Basura:\",round(len(df[df['Encode']==0])/len(df)*100,2),\"%\")\n",
    "print(\"Porcentaje de Tablets:\",round(len(df[df['Encode']==1])/len(df)*100,2),\"%\")\n",
    "print(\"Porcentaje de PC:\",round(len(df[df['Encode']==2])/len(df)*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "max(vect.get_feature_names(), key=lambda token:len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "predictions = clf.predict(vect.transform(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print('F1 Measure: %.3f' % f1_score(y_test.astype(int),predictions, average='micro'))\n",
    "print('Precision: %.3f' % precision_score(y_test.astype(int),predictions, average='micro'))\n",
    "print('F1 Measure: %.3f' % recall_score(y_test.astype(int),predictions, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_db\n",
    "finalNB = result.join(pd.DataFrame(predictions, columns = (['Predictions Countvectorizer'])))\n",
    "finalNB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_db\n",
    "finalTfidf = result.join(pd.DataFrame(predictions, columns = (['Predictions TFIDF'])))\n",
    "finalTfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "model = LinearSVC()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 Measure: %.3f' % f1_score(y_test.astype(int),predictions, average='micro'))\n",
    "print('Precision: %.3f' % precision_score(y_test.astype(int),predictions, average='micro'))\n",
    "print('F1 Measure: %.3f' % recall_score(y_test.astype(int),predictions, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = test_db\n",
    "final_SVC = result.join(pd.DataFrame(predictions, columns = (['Predictions LinearSVC'])))\n",
    "final_SVC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('F1 Measure: %.3f' % f1_score(y_test.astype(int),predictions, average='micro'))\n",
    "print('Precision: %.3f' % precision_score(y_test.astype(int),predictions, average='micro'))\n",
    "print('F1 Measure: %.3f' % recall_score(y_test.astype(int),predictions, average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test.astype(int),predictions, target_names=df['Type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(y_test.astype(int),predictions)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=df['Type'].unique(), yticklabels=df['Type'].unique())\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "trash_out = r'C:\\Users\\Gpardo\\Desktop\\Clasification\\test' \n",
    "save = datetime.datetime.now().strftime(\"%y_%m_%d\")\n",
    "dir = trash_out +\"_\"+str(save)+\".xlsx\"\n",
    "print(dir)\n",
    "\n",
    "writer = pd.ExcelWriter(dir, engine='xlsxwriter')\n",
    "\n",
    "#Convert the dataframe to an XlsxWriter Excel object.\n",
    "finalNB.to_excel(writer, sheet_name='NB-countvect')\n",
    "finalTfidf.to_excel(writer, sheet_name='NB-TFIDF')\n",
    "final_SVC.to_excel(writer, sheet_name='SVC-TFIDF')\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the classiffier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_test['Descripcion_Test'] = db_test['Descripcion_Clean'].apply(lambda x : filter(None,x.split(\" \"))).apply(lambda x : [stemmer.stem(y) for y in x]).apply(lambda x : \" \".join(x))\n",
    "db_test['Index Field 1'] = mergefield(db_test,\" \") #11,12,13,14 test-> 17,12,13,14\n",
    "db_test['Index Field 1'] = db_test['Index Field 1'].str.replace('|'.join(esp_list), \"\", case=False).str.replace('\\s\\s+', \" \", case=False)\n",
    "db_test.head()\n",
    "x_new = db_test[['Index Field 1','Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new.loc[x_new['Type']==\"Trash\",'Encode']= \"0\"\n",
    "x_new.loc[x_new['Type']==\"Tablet\",'Encode']= \"1\"\n",
    "x_new.loc[x_new['Type']==\"PC\",'Encode']= \"2\"\n",
    "x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a unique DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_new.columns = ['Words', 'Type', 'Encode']\n",
    "complete_db = x_new.append(df)\n",
    "np.random.seed(0)\n",
    "complete_db = complete_db.reset_index(drop=True)\n",
    "complete_db = complete_db.reindex(np.random.permutation(complete_db.index))\n",
    "complete_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = complete_db['Words'].str.lower()\n",
    "labels = complete_db['Encode'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, vect.transform(features), labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    vect.transform(features), labels, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred, target_names=df['Type'].unique()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.5, random_state=0)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=3, ngram_range=(1, 2)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "model = SVC(C=1000, gamma=0.001,kernel='rbf')\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('F1 Measure: %.3f' % f1_score(y_test.astype(int),predictions, average='micro'))\n",
    "print('Precision: %.3f' % precision_score(y_test.astype(int),predictions, average='micro'))\n",
    "print('F1 Measure: %.3f' % recall_score(y_test.astype(int),predictions, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C:\\Users\\Gpardo\\Downloads\\FInal DB PCD v1.xlsx\n",
    "db_hist_tablet = loading_excel() #Modelo\n",
    "db_hist_trash = loading_excel()\n",
    "db_hist_pc = loading_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_hist_tablet #Modelo\n",
    "#db_hist_trash\n",
    "db_hist_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tablets = db_hist_tablet[['Period','Descripcion_Clean','Test Vendors.1','Modelo_1.1','Model ID']]\n",
    "Tablets.columns = ['Period', 'Descripcion', 'Vendors', 'Modelo',\n",
    "       'Model ID']\n",
    "Tablets['Type'] = 'Tablets'\n",
    "Tablets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC = db_hist_pc[['Period','Descripcion_Clean','Test Vendors','Modelo_1','Model ID']]\n",
    "PC.columns = ['Period', 'Descripcion', 'Vendors', 'Modelo',\n",
    "       'Model ID']\n",
    "PC['Type'] = 'PCS'\n",
    "PC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Trash = db_hist_trash[['Period','Descripcion_Clean','Test Vendors','Modelo_1','Model ID']]\n",
    "Trash.columns = ['Period', 'Descripcion', 'Vendors', 'Modelo',\n",
    "       'Model ID']\n",
    "Trash['Type'] = 'Trash'\n",
    "Trash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New = db_test[['Timescale','Descripcion_Clean','Test Vendors','Modelo_1','Model ID','Type']]\n",
    "New.columns = ['Period', 'Descripcion', 'Vendors', 'Modelo',\n",
    "       'Model ID','Type']\n",
    "New.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = Tablets.append(PC).append(Trash)\n",
    "df2 = df1.append(New)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "trash_out = r'C:\\Users\\Gpardo\\Desktop\\Clasification\\DB_Classifier' \n",
    "save = datetime.datetime.now().strftime(\"%y_%m_%d\")\n",
    "dir = trash_out +\"_\"+str(save)+\".xlsx\"\n",
    "print(dir)\n",
    "\n",
    "writer = pd.ExcelWriter(dir, engine='xlsxwriter')\n",
    "\n",
    "#Convert the dataframe to an XlsxWriter Excel object.\n",
    "df2.to_excel(writer, sheet_name='Final Database')\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
